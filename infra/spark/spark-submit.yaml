# infra/spark/spark-submit.yaml
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: funnel-analysis
  namespace: apps-async # Hardened: Deployed to dedicated async workload namespace
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "gcr.io/spark-operator/spark-py:v3.1.1"
  mainApplicationFile: "local:///opt/spark/jobs/funnel_analysis.py"
  sparkVersion: "3.1.1"
  
  # Restart Policy for Batch
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 5

  # Driver Resources
  driver:
    cores: 1
    coreLimit: "1000m"
    memory: "512m"
    serviceAccount: spark-sa
    env:
      - name: DB_URL
        value: "jdbc:postgresql://postgres-primary.data-postgres.svc.cluster.local:5432/ecommerce_db"
      - name: DB_USER
        valueFrom:
          secretKeyRef:
            name: postgres-creds
            key: username
      - name: DB_PASSWORD
        valueFrom:
          secretKeyRef:
            name: postgres-creds
            key: password

  # Executor Resources
  executor:
    cores: 1
    instances: 2
    memory: "1024m" # Increased for stability
    env:
      - name: DB_URL
        value: "jdbc:postgresql://postgres-primary.data-postgres.svc.cluster.local:5432/ecommerce_db"
      - name: DB_USER
        valueFrom:
          secretKeyRef:
            name: postgres-creds
            key: username
      - name: DB_PASSWORD
        valueFrom:
          secretKeyRef:
            name: postgres-creds
            key: password

  deps:
    jars:
      - "local:///opt/spark/jars/postgresql-42.2.18.jar"
      - "local:///opt/spark/jars/gcs-connector-hadoop2-2.2.0.jar"
